@misc{Xiang1997,
	abstract = {In learning belief networks, the single link lookahead search is widely adopted to reduce the search space. We show that there exists a class of probabilistic domain models which displays a special pattern of dependency. We analyze the behavior of several learning algorithms using diierent scoring metrics such as the entropy, conditional independence, minimal description length and Bayesian metrics. We demonstrate that single link lookahead search procedures (employed in these algorithms) cannot learn these models correctly. Thus, when the underlying domain model actually belongs to this class, the use of a single link search procedure will result in learning of an incorrect model. This may lead to inference errors when the model is used. Our analysis suggests that if the prior knowledge about a domain does not rule out the possible existence of these models, a multi-link lookahead search or other heuristics should be used for the learning process.},
	author = {Yuanfei Xiang and Y Xiang and S K M Wong},
	title = {Critical Remarks on Single Link Search in Learning Belief Networks},
	url = {https://www.researchgate.net/publication/2405133},
	year = {1997},
}
@misc{Xiang1999,
	abstract = {Learning belief networks from large domains can be expensive even with single-link lookahead search (SLLS). Since a SLLS cannot learn correctly in a class of problem domains, multi-link lookahead search (MLLS) is needed which further increases the computational complexity. In our experiment, learning in some difficult domains over more than a dozen variables took days. In this paper, we study how to use parallelism to speed up SLLS for learning in large domains and to tackle the increased complexity of MLLS for learning in difficult domains. We propose a natural decomposition of the learning task for parallel processing. We investigate two strategies for job allocation among processors to further improve load balancing and efficiency of the parallel system. For learning from very large datasets, we present a regrouping of the available processors such that slow data access through the file system can be replaced by fast memory access. Experimental results in a distributed memory MIMD computer demonstrate the effectiveness of the proposed algorithms.},
	author = {Y Xiang and T Chu and Yike Guo and Robert Grossman},
	journal = {Data Mining and Knowledge Discovery},
	keywords = {belief networks,parallel implementation of data mining},
	pages = {315-339},
	title = {Parallel Learning of Belief Networks in Large and Difficult Domains},
	volume = {3},
	year = {1999},
}
@inbook{Heckerman2007,
	abstract = {Bayesian methods have been developed for learning Bayesian networks from data. Most of this work has concentrated on Bayesian networks interpreted as a representation of probabilistic conditional independence without considering causation. Other researchers have shown that having a causal interpretation can be important because it allows us to predict the effects of interventions in a domain. In this chapter, we extend Bayesian methods for learning acausal Bayesian networks to causal Bayesian networks.},
	author = {David Heckerman},
	doi = {10.1017/CBO9780511611308.012},
	isbn = {9780511611308},
	journal = {Advances in Decision Analysis: From Foundations to Applications},
	month = {1},
	pages = {202-220},
	publisher = {Cambridge University Press},
	title = {A bayesian approach to learning causal networks},
	year = {2007},
}
@misc{,
	abstract = {This paper presents a procedure for the construction of probabilistic networks from a database of observations based on the minimum description length principle. On top of the advantages of the Bayesian approach the minimum description length principle offers the advantage that every probabilistic network structure that represents the same set of independencies gets assigned the same quality. This makes it is very suitable for the order.optimization procedure as described in [4]. Preliminary test results show that the algorithm performs comparable to the algorithm based on the Bayesian approach [6].},
	author = {Remco R Bouckaert},
	title = {Probabilistic Network Construction Using the Minimum Description Length PrinCiple},
}
@misc{Chickering2004,
	abstract = {In this paper, we provide new complexity results for algorithms that learn discrete-variable Bayesian networks from data. Our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest structure for which the model is able to represent the generative distribution exactly. Our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset. We show that identifying high-scoring structures is NP-hard, even when any combination of one or more of the following hold: the generative distribution is perfect with respect to some DAG containing hidden variables; we are given an independence oracle; we are given an inference oracle; we are given an information oracle; we restrict potential solutions to structures in which each node has at most k parents, for all k ≥ 3. Our proof relies on a new technical result that we establish in the appendices. In particular, we provide a method for constructing the local distributions in a Bayesian network such that the resulting joint distribution is provably perfect with respect to the structure of the network.},
	author = {David Maxwell Chickering and David Heckerman and Christopher Meek},
	journal = {Journal of Machine Learning Research},
	keywords = {NP-Hard,large-sample data,learning Bayesian networks,search complexity},
	pages = {1287-1330},
	title = {Large-Sample Learning of Bayesian Networks is NP-Hard},
	volume = {5},
	year = {2004},
}
@misc{Cooper1992,
	abstract = {This paper presents a Bayesian method for constructing probabilistic networks from databases. In particular , we focus on constructing Bayesian belief networks. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabifistic expert systems. We extend the basic method to handle missing data and hidden (latent) variables. We show how to perform probahilistic inference by averaging over the inferences of multiple belief networks. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. Finally, we relate the methods in this paper to previous work, and we discuss open problems. Keywords. probabilistic networks, Bayesian belief networks, machine learning, induction 1. I n t r o d u c t i o n In this paper, we present a Bayesian method for constructing a probabilistic network from a database of records, which we call cases. Once constructed, such a network can provide insight into probabilistic dependencies that exist among the variables in the database. One application is the automated discovery of dependency relationships. The computer program searches for a probabilistic-network structure that has a high posterior probability given the database, and outputs the structure and its probability. A related task is computer-assisted hypothesis testing: The user enters a hypothetical structure of the dependency relationships among a set of variables, and the program calculates the probability of the structure given a database of cases on the variables. We can also construct a network and use it for computer-based diagnosis. For example, suppose we have a database in which a case contains data about the behavior of some system (i.e., findings). Suppose further that a case contains data about whether this particular behavior follows from proper system operation, or alternatively, is caused by one of several possible faults. Assume that the database contains many such cases from previous episodes of proper and faulty behavior. The method that we present in this paper can be used to construct from the database a probabilistic network that captures the probabilistic dependencies among findings and faults. Such a network then can be applied to classify future cases of system behavior by assigning a posterior probability to each of the possible faults and to the event "proper system operation." In this paper, we also shall discuss diagnostic inference that is based on combining the inferences of multiple alternative networks.},
	author = {Gregory E Cooper},
	pages = {309-347},
	title = {A Bayesian Method for the Induction of Probabilistic Networks from Data},
	volume = {9},
	year = {1992},
}
@article{Cai2016,
	abstract = {Bayesian network (BN) is a commonly used tool in probabilistic reasoning of uncertainty in industrial processes, but it requires modeling of large and complex systems, in situations such as fault diagnosis and reliability evaluation. Motivated by reduction of the overall complexities of BNs for fault diagnosis, and the reporting of faults that immediately occur, a real-time fault diagnosis methodology of complex systems with repetitive structures is proposed using object-oriented Bayesian networks (OOBNs). The modeling methodology consists of two main phases: an off-line OOBN construction phase and an on-line fault diagnosis phase. In the off-line phase, sensor historical data and expert knowledge are collected and processed to determine the faults and symptoms, and OOBN-based fault diagnosis models are developed subsequently. In the on-line phase, operator experience and sensor real-time data are placed in the OOBNs to perform the fault diagnosis. According to engineering experience, the judgment rules are defined to obtain the fault diagnosis results.},
	author = {Baoping Cai and Hanlin Liu and Min Xie},
	doi = {10.1016/j.ymssp.2016.04.019},
	issn = {10961216},
	journal = {Mechanical Systems and Signal Processing},
	keywords = {Complex systems,Fault diagnosis,Object-oriented Bayesian networks,Real-time},
	month = {12},
	pages = {31-44},
	publisher = {Academic Press},
	title = {A real-time fault diagnosis methodology of complex systems using object-oriented Bayesian networks},
	volume = {80},
	year = {2016},
}
@article{Kasper2012,
	abstract = {This article introduces a novel approach towards the recognition of typical driving maneuvers in structured highway scenarios and shows some key benefits of traffic scene modeling with object-oriented Bayesian networks (OOBNs). The approach exploits the advantages of an introduced lane-related coordinate system together with individual occupancy schedule grids for all modeled vehicles. This combination allows an efficient classification of the existing vehicle-lane and vehicle- vehicle relations in traffic scenes and thus substantially improves the understanding of complex traffic scenes. Probabilities and variances within the network are propagated systematically which results in probabilistic sets of the modeled driving maneuvers. Using this generic approach, the network is able to classify a total of 27 driving maneuvers including merging and object following.},
	author = {Dietmar Kasper and Galia Weidl and Thao Dang and Gabi Breuel and Andreas Tamke and Andreas Wedel and Wolfgang Rosenstiel},
	doi = {10.1109/mits.2012.2203229},
	issn = {1939-1390},
	issue = {3},
	journal = {IEEE Intelligent Transportation Systems Magazine},
	month = {8},
	pages = {19-31},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	title = {Object-Oriented Bayesian Networks for Detection of Lane Change Maneuvers},
	volume = {4},
	year = {2012},
}
@inproceedings{Weber2006,
	abstract = {Nowadays, the complex manufacturing processes have to be dynamically modelled and controlled to optimise the diagnosis and the maintenance policies. This article presents a methodology that will help developing Dynamic Object Oriented Bayesian Networks (DOOBNs) to formalise such complex dynamic models. The goal is to have a general reliability evaluation of a manufacturing process, from its implementation to its operating phase. The added value of this formalisation methodology consists in using the a priori knowledge of both the system's functioning and malfunctioning. Networks are built on principles of adaptability and integrate uncertainties on the relationships between causes and effects. Thus, the purpose is to evaluate, in terms of reliability, the impact of several decisions on the maintenance of the system. This methodology has been tested, in an industrial context, to model the reliability of a water (immersion) heater system. © 2005 Elsevier Ltd. All rights reserved.},
	author = {Philippe Weber and Lionel Jouffe},
	doi = {10.1016/j.ress.2005.03.006},
	issn = {09518320},
	issue = {2},
	journal = {Reliability Engineering and System Safety},
	keywords = {Dynamic Object Oriented Bayesian Networks (DOOBNs),Markov Chain,Reliability estimation},
	month = {2},
	pages = {149-162},
	title = {Complex system reliability modelling with Dynamic Object Oriented Bayesian Networks (DOOBN)},
	volume = {91},
	year = {2006},
}
@misc{,
	abstract = {Learning disentanglement aims at finding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identifiabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identified with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through "do-operation" to the causal factors.},
	author = {Mengyue Yang and Furui Liu and Zhitang Chen and Xinwei Shen and Jianye Hao and Jun Wang},
	title = {CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models},
}
@article{,
	title = {報告書},
}
@article{,
	abstract = {The measure of the equitable and sustainable well-being (Bes) is of growing interest in the last years. The National Institute of Statistics (Istat) provides, for Italy, a wide set of indicators describing each domain of well-being that is, by definition, a multidimensional concept. In this study, we propose the use of Bayesian networks to deal with basic and composite Bes indicators. Its capability to model very complex multivariate dependence structures is useful to describe the relationships between indicators belonging to different domains and, being a probabilistic expert system, the estimated network could be also useful for probabilistic inference and what-if analysis. In this study, all the Bayesian networks structures have been estimated by means of the hill climbing algorithm based on bootstrap resampling and model averaging in order to prevent bias due to deviations from the normality assumption.},
	author = {Pierpaolo D’Urso and Vincenzina Vitale},
	doi = {10.1007/s11205-020-02401-z},
	issn = {15730921},
	issue = {3},
	journal = {Social Indicators Research},
	keywords = {Bayesian networks,Bootstrap resampling,Equitable and sustainable well-being,Structural learning},
	month = {10},
	pages = {897-919},
	publisher = {Springer Science+Business Media B.V.},
	title = {Bayesian Networks Model Averaging for Bes Indicators},
	volume = {151},
	year = {2020},
}
@misc{,
	abstract = {For classification problems, Bayesian networks are often used to infer a class variable when given feature variables. Earlier reports have described that the classification accuracy of Bayesian network structures achieved by maximizing the marginal likelihood (ML) is lower than that achieved by maximizing the conditional log likelihood (CLL) of a class variable given the feature variables. However, the performance of Bayesian network structures achieved by maximizing ML is not necessarily worse than that achieved by maximizing CLL for large data because ML has asymptotic consistency. As the sample size becomes small, however, the error of learning structures by maximizing the ML becomes rapidly large; it then degrades the classification accuracy. As a method to resolve this shortcoming, model averaging, which marginalizes the class variable posterior over all structures, has been proposed. However, the posterior standard error of the structures in the model averaging becomes large as the sample size becomes small; it subsequently degrades the classification accuracy. The main idea of this study is to improve the classification accuracy using the subbagging to reduce the posterior standard error of the structures in the model averaging. Moreover, to guarantee asymptotic consistency, we use the K-best method with the ML score. The experimentally obtained results demonstrate that our proposed method provides more accurate classification for small data than earlier methods do.},
	author = {Shouta Sugahara and Itsuki Aomi and Maomi Ueno},
	keywords = {Bayesian networks,classification,model averaging,structure learning},
	title = {Bayesian Network Model Averaging Classifiers by Subbagging},
}
@misc{Cooper2004,
	abstract = {In this paper 1 we consider the problem of performing Bayesian model-averaging over a class of discrete Bayesian network structures consistent with a partial ordering and with bounded in-degree k. We show that for N nodes this class contains in the worst-case at least Ω(N/2 k N/2) distinct network structures, and yet model averaging over these structures can be performed using O(N k · N) operations. Furthermore we show that there exists a single Bayesian network that defines a joint distribution over the variables that is equivalent to model averaging over these structures. Although constructing this network is computationally prohibitive, we show that it can be approximated by a tractable network, allowing approximate model-averaged probability calculations to be performed in O(N) time. Our result also leads to an exact and linear-time solution to the problem of averaging over the 2 N possible feature sets in a naïve Bayes model, providing an exact Bayesian solution to the troublesome feature-selection problem for naïve Bayes classifiers. We demonstrate the utility of these techniques in the context of supervised classification, showing empirically that model averaging consistently beats other generative Bayesian-network-based models, even when the generating model is not guaranteed to be a member of the class being averaged over. We characterize the performance over several parameters on simulated and real-world data.},
	author = {Gregory F Cooper},
	journal = {Journal of Machine Learning Research},
	keywords = {Bayesian model averaging,Bayesian networks,classification,feature selection,naïve Bayes classifiers},
	pages = {1177-1203},
	title = {Model Averaging for Prediction with Discrete Bayesian Networks Denver Dash},
	volume = {5},
	year = {2004},
}
@article{Liao2020,
	abstract = {A Bayesian network is a widely used probabilistic graphical model with applications in knowledge discovery and prediction. Learning a Bayesian network (BN) from data can be cast as an optimization problem using the well-known score-and-search approach. However, selecting a single model (i.e., the best scoring BN) can be misleading or may not achieve the best possible accuracy. An alternative to committing to a single model is to perform some form of Bayesian or frequentist model averaging, where the space of possible BNs is sampled or enumerated in some fashion. Unfortunately, existing approaches for model averaging either severely restrict the structure of the Bayesian network or have only been shown to scale to networks with fewer than 30 random variables. In this paper, we propose a novel approach to model averaging inspired by performance guarantees in approximation algorithms. Our approach has two primary advantages. First, our approach only considers credible models in that they are optimal or near-optimal in score. Second, our approach is more efficient and scales to significantly larger Bayesian networks than existing approaches.},
	author = {Zhenyu A. Liao and Charupriya Sharma and James Cussens and Peter van Beek},
	month = {8},
	title = {Learning All Credible Bayesian Network Structures for Model Averaging},
	url = {http://arxiv.org/abs/2008.13618},
	year = {2020},
}
@article{Broom2012,
	abstract = {Considerable progress has been made on algorithms for learning the structure of Bayesian networks from data. Model averaging by using bootstrap replicates with feature selection by thresholding is a widely used solution for learning features with high confidence. Yet, in the context of limited data many questions remain unanswered. What scoring functions are most effective for model averaging? Does the bias arising from the discreteness of the bootstrap significantly affect learning performance? Is it better to pick the single best network or to average multiple networks learnt from each bootstrap resample? How should thresholds for learning statistically significant features be selected? The best scoring functions are Dirichlet Prior Scoring Metric with small λ and the Bayesian Dirichlet metric. Correcting the bias arising from the discreteness of the bootstrap worsens learning performance. It is better to pick the single best network learnt from each bootstrap resample. We describe a permutation based method for determining significance thresholds for feature selection in bagged models. We show that in contexts with limited data, Bayesian bagging using the Dirichlet Prior Scoring Metric (DPSM) is the most effective learning strategy, and that modifying the scoring function to penalize complex networks hampers model averaging. We establish these results using a systematic study of two well-known benchmarks, specifically ALARM and INSURANCE. We also apply our network construction method to gene expression data from the Cancer Genome Atlas Glioblastoma multiforme dataset and show that survival is related to clinical covariates age and gender and clusters for interferon induced genes and growth inhibition genes. For small data sets, our approach performs significantly better than previously published methods.},
	author = {Bradley M. Broom and Kim Anh Do and Devika Subramanian},
	doi = {10.1186/1471-2105-13-S13-S10},
	issn = {14712105},
	journal = {BMC bioinformatics},
	pmid = {23320818},
	title = {Model averaging strategies for structure learning in Bayesian networks with limited data.},
	volume = {13 Suppl 13},
	year = {2012},
}
@article{Sugahara2022,
	abstract = {When applied to classification problems, Bayesian networks are often used to infer a class variable when given feature variables. Earlier reports have described that the classification accuracy of Bayesian network structures achieved by maximizing the marginal likelihood (ML) is lower than that achieved by maximizing the conditional log likelihood (CLL) of a class variable given the feature variables. Nevertheless, because ML has asymptotic consistency, the performance of Bayesian network structures achieved by maximizing ML is not necessarily worse than that achieved by maximizing CLL for large data. However, the error of learning structures by maximizing the ML becomes much larger for small sample sizes. That large error degrades the classification accuracy. As a method to resolve this shortcoming, model averaging has been proposed to marginalize the class variable posterior over all structures. However, the posterior standard error of each structure in the model averaging becomes large as the sample size becomes small; it subsequently degrades the classification accuracy. The main idea of this study is to improve the classification accuracy using subbagging, which is modified bagging using random sampling without replacement, to reduce the posterior standard error of each structure in model averaging. Moreover, to guarantee asymptotic consistency, we use the K-best method with the ML score. The experimentally obtained results demonstrate that our proposed method provides more accurate classification than earlier BNC methods and the other state-of-the-art ensemble methods do.},
	author = {Shouta Sugahara and Itsuki Aomi and Maomi Ueno},
	doi = {10.3390/e24050743},
	issn = {10994300},
	issue = {5},
	journal = {Entropy},
	keywords = {Bayesian networks,classification,model averaging,structure learning},
	month = {5},
	publisher = {MDPI},
	title = {Bayesian Network Model Averaging Classifiers by Subbagging},
	volume = {24},
	year = {2022},
}
@misc{Scanagatta2019,
	abstract = {A necessary step in the development of artificial intelligence is to enable a machine to represent how the world works, building an internal structure from data. This structure should hold a good trade-off between expressive power and querying efficiency. Bayesian networks have proven to be an effective and versatile tool for the task at hand. They have been applied to modeling knowledge in a variety of fields, ranging from bioinformatics to law, from image processing to economic risk analysis. A crucial aspect is learning the dependency graph of a Bayesian network from data. This task, called structure learning, is NP-hard and is the subject of intense, cutting-edge research. In short, it can be thought of as choosing one graph over the many candidates, grounding our reasoning over a collection of samples of the distribution generating the data. The number of possible graphs increases very quickly at the increase in the number of variables. Searching in this space, and selecting a graph over the others, becomes quickly burdensome. In this survey, we review the most relevant structure learning algorithms that have been proposed in the literature. We classify them according to the approach they follow for solving the problem and we also show alternatives for handling missing data and continuous variable. An extensive review of existing software tools is also given.},
	author = {Mauro Scanagatta and Antonio Salmerón and Fabio Stella},
	doi = {10.1007/s13748-019-00194-y},
	issn = {21926360},
	issue = {4},
	journal = {Progress in Artificial Intelligence},
	keywords = {Bayesian network,Machine learning,Statistics,Structure learning},
	month = {12},
	pages = {425-439},
	publisher = {Springer Verlag},
	title = {A survey on Bayesian network structure learning from data},
	volume = {8},
	year = {2019},
}
@article{Daly2011,
	abstract = {Bayesian networks have become a widely used method in the modelling of uncertain knowledge. Owing to the difficulty domain experts have in specifying them, techniques that learn Bayesian networks from data have become indispensable. Recently, however, there have been many important new developments in this field. This work takes a broad look at the literature on learning Bayesian networks-in particular their structure-from data. Specific topics are not focused on in detail, but it is hoped that all the major fields in the area are covered. This article is not intended to be a tutorial-for this, there are many books on the topic, which will be presented. However, an effort has been made to locate all the relevant publications, so that this paper can be used as a ready reference to find the works on particular sub-topics. © 2011 Cambridge University Press.},
	author = {Rónán Daly and Qiang Shen and Stuart Aitken},
	doi = {10.1017/S0269888910000251},
	issn = {02698889},
	issue = {2},
	journal = {Knowledge Engineering Review},
	month = {6},
	pages = {99-157},
	title = {Learning Bayesian networks: Approaches and issues},
	volume = {26},
	year = {2011},
}
@misc{,
	abstract = {One of the basic tasks for Bayesian networks (BNs) is that of learning a network structure from data. The BN-learning problem is NP-hard, so the standard solution is heuristic search. Many approaches have been proposed for this task, but only a very small number outperform the baseline of greedy hill-climbing with tabu lists; moreover, many of the proposed algorithms are quite complex and hard to implement. In this paper, we propose a very simple and easy-to-implement method for addressing this task. Our approach is based on the well-known fact that the best network (of bounded in-degree) consistent with a given node ordering can be found very efficiently. We therefore propose a search not over the space of structures, but over the space of orderings, selecting for each ordering the best network consistent with it. This search space is much smaller, makes more global search steps, has a lower branching factor, and avoids costly acyclicity checks. We present results for this algorithm on both synthetic and real data sets, evaluating both the score of the network found and in the running time. We show that ordering-based search outperforms the standard baseline, and is competitive with recent algorithms that are much harder to implement.},
	author = {Marc Teyssier and Daphne Koller},
	title = {Ordering-Based Search: A Simple and Effective Algorithm for Learning Bayesian Networks},
}
@article{Wang2021,
	abstract = {It is a long-standing question to discover causal relations among a set of variables in many empirical sciences. Recently, Reinforcement Learning (RL) has achieved promising results in causal discovery from observational data. However, searching the space of directed graphs and enforcing acyclicity by implicit penalties tend to be inefficient and restrict the existing RL-based method to small scale problems. In this work, we propose a novel RL-based approach for causal discovery, by incorporating RL into the ordering-based paradigm. Specifically, we formulate the ordering search problem as a multi-step Markov decision process, implement the ordering generating process with an encoder-decoder architecture, and finally use RL to optimize the proposed model based on the reward mechanisms designed for~each ordering. A generated ordering would then be processed using variable selection to obtain the final causal graph. We analyze the consistency and computational complexity of the proposed method, and empirically show that a pretrained model can be exploited to accelerate training. Experimental results on both synthetic and real data sets shows that the proposed method achieves a much improved performance over existing RL-based method.},
	author = {Xiaoqiang Wang and Yali Du and Shengyu Zhu and Liangjun Ke and Zhitang Chen and Jianye Hao and Jun Wang},
	month = {5},
	title = {Ordering-Based Causal Discovery with Reinforcement Learning},
	url = {http://arxiv.org/abs/2105.06631},
	year = {2021},
}
@article{Marella2014,
	abstract = {In this paper we propose to use the object-oriented Bayesian network (OOBN) architecture to model measurement errors. We then apply our model to the Italian survey on household income and wealth (SHIW) 2008. Attention is focused on errors caused by the respondents. The parameters of the error model are estimated using a validation sample. The network is used to stochastically impute micro data for households. In particular imputation is performed also using an auxiliary variable. Indices are calculated to evaluate the performance of the correction procedure and show that accounting for auxiliary information improves the results. Finally, potentialities and possible extensions of the Bayesian network approach both to the measurement error context and to official statistics problems in general are discussed.},
	author = {Daniela Marella and Paola Vicard},
	doi = {10.1007/978-3-319-05320-2__9},
	keywords = {Bayesian networks,Measurement error,Respondent error},
	title = {Modelling Measurement Errors by Object-Oriented Bayesian Networks: An Application to 2008 SHIW},
	year = {2014},
}
@article{Musella2015,
	abstract = {Quality management and customer satisfaction evaluation can be difficult tasks to perform when processes involve multiple production lines or provide multichannel services. As a consequence, the top management needs to analyse the problem from different perspectives, to evaluate possible improvement strategies at several levels and to take appropriate decisions. To this aim, we propose to use object-oriented Bayesian networks by which different quality aspects and evaluations can be integrated in a unique framework allowing to analyse improvement strategies in real time. We show, by an application to an internal-customer satisfaction survey, how to combine the perceived quality of different production areas and how to evaluate the impact on the global quality of improvement actions developed in one or more areas.},
	author = {Flaminia Musella and Paola Vicard},
	doi = {10.1007/s11135-013-9977-3},
	issn = {15737845},
	issue = {1},
	journal = {Quality and Quantity},
	keywords = {Bayesian networks,Decision making,Global quality evaluation,What-if analysis},
	month = {1},
	pages = {115-133},
	publisher = {Springer Netherlands},
	title = {Object-oriented Bayesian networks for complex quality management problems},
	volume = {49},
	year = {2015},
}
@misc{,
	abstract = {Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of pro­ gramming using logical circuits. In this paper, we de­ scribe an object-oriented Bayesian network (OOBN) lan­ guage, which allows complex domains to be described in terms of interrelated objects. We use a Bayesian net­ work fragment to describe the probabilistic relations be­ tween the attributes of an object. These attributes can themsel ves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to pro­ vide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inher­ itance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language has clear declarative semantics: an OOBN can be interpreted as a stochas­ tic functional program, so that it uniquely specifies a probabilistic model. We provide an inference algorithm for OOBNs, and show that much of the structural infor­ mation encoded by an OOBN-particularly the encap­ sulation of variables within an object and the reuse of model fragments in different contexts-can also be used to speed up the inference process.},
	author = {Daphne Koller},
	title = {Object-Oriented Bayesian Networks},
}
@misc{,
	abstract = {Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability-that is, the Bayesian score-of such a network , given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally , we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function.},
	author = {David Maxwell Chickering and David Heckerman},
	title = {A Bayesian Approach to Learning Bayesian Networks with Local Structure},
}
@misc{,
	author = {M Jordan and J Kleinberg and B Schölkopf},
	title = {Information Science and Statistics},
}
@article{Lachapelle2019,
	abstract = {We propose a novel score-based approach to learning a directed acyclic graph (DAG) from observational data. We adapt a recently proposed continuous constrained optimization formulation to allow for nonlinear relationships between variables using neural networks. This extension allows to model complex interactions while avoiding the combinatorial nature of the problem. In addition to comparing our method to existing continuous optimization methods, we provide missing empirical comparisons to nonlinear greedy search methods. On both synthetic and real-world data sets, this new method outperforms current continuous methods on most tasks, while being competitive with existing greedy search methods on important metrics for causal inference.},
	author = {Sébastien Lachapelle and Philippe Brouillard and Tristan Deleu and Simon Lacoste-Julien},
	month = {6},
	title = {Gradient-Based Neural DAG Learning},
	url = {http://arxiv.org/abs/1906.02226},
	year = {2019},
}
@article{Zhang2019,
	abstract = {Graph structured data are abundant in the real world. Among different graph types, directed acyclic graphs (DAGs) are of particular interest to machine learning researchers, as many machine learning models are realized as computations on DAGs, including neural networks and Bayesian networks. In this paper, we study deep generative models for DAGs, and propose a novel DAG variational autoencoder (D-VAE). To encode DAGs into the latent space, we leverage graph neural networks. We propose an asynchronous message passing scheme that allows encoding the computations on DAGs, rather than using existing simultaneous message passing schemes to encode local graph structures. We demonstrate the effectiveness of our proposed DVAE through two tasks: neural architecture search and Bayesian network structure learning. Experiments show that our model not only generates novel and valid DAGs, but also produces a smooth latent space that facilitates searching for DAGs with better performance through Bayesian optimization.},
	author = {Muhan Zhang and Shali Jiang and Zhicheng Cui and Roman Garnett and Yixin Chen},
	month = {4},
	title = {D-VAE: A Variational Autoencoder for Directed Acyclic Graphs},
	url = {http://arxiv.org/abs/1904.11088},
	year = {2019},
}
@article{,
	title = {DBN},
}
@article{Zheng2018,
	abstract = {Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: We formulate the structure learning problem as a purely \emph\{continuous\} optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree. Code implementing the proposed algorithm is open-source and publicly available at https://github.com/xunzheng/notears.},
	author = {Xun Zheng and Bryon Aragam and Pradeep Ravikumar and Eric P. Xing},
	month = {3},
	title = {DAGs with NO TEARS: Continuous Optimization for Structure Learning},
	url = {http://arxiv.org/abs/1803.01422},
	year = {2018},
}
@misc{,
	abstract = {We propose a new differentiable probabilistic model over DAGs (DP-DAG). DP-DAG allows fast and differentiable DAG sampling suited to continuous optimization. To this end, DP-DAG samples a DAG by successively (1) sampling a linear ordering of the node and (2) sampling edges consistent with the sampled linear ordering. We further propose VI-DP-DAG, a new method for DAG learning from observational data which combines DP-DAG with variational inference. Hence, VI-DP-DAG approximates the posterior probability over DAG edges given the observed data. VI-DP-DAG is guaranteed to output a valid DAG at any time during training and does not require any complex augmented Lagrangian optimization scheme in contrast to existing differentiable DAG learning approaches. In our extensive experiments, we compare VI-DP-DAG to other differentiable DAG learning baselines on synthetic and real datasets. VI-DP-DAG significantly improves DAG structure and causal mechanism learning while training faster than competitors.},
	author = {Bertrand Charpentier and Simon Kibler and Stephan Günnemann},
	title = {DIFFERENTIABLE DAG SAMPLING},
}
@article{Yu2019,
	abstract = {Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \url\{https://github.com/fishmoon1234/DAG-GNN\}.},
	author = {Yue Yu and Jie Chen and Tian Gao and Mo Yu},
	month = {4},
	title = {DAG-GNN: DAG Structure Learning with Graph Neural Networks},
	url = {http://arxiv.org/abs/1904.10098},
	year = {2019},
}
@article{Vowels2021,
	abstract = {Causal reasoning is a crucial part of science and human intelligence. In order to discover causal relationships from data, we need structure discovery methods. We provide a review of background theory and a survey of methods for structure discovery. We primarily focus on modern, continuous optimization methods, and provide reference to further resources such as benchmark datasets and software packages. Finally, we discuss the assumptive leap required to take us from structure to causality.},
	author = {Matthew J. Vowels and Necati Cihan Camgoz and Richard Bowden},
	month = {3},
	title = {D'ya like DAGs? A Survey on Structure Learning and Causal Discovery},
	url = {http://arxiv.org/abs/2103.02582},
	year = {2021},
}
@article{Kitson2023,
	abstract = {Bayesian Networks (BNs) have become increasingly popular over the last few decades as a tool for reasoning under uncertainty in fields as diverse as medicine, biology, epidemiology, economics and the social sciences. This is especially true in real-world areas where we seek to answer complex questions based on hypothetical evidence to determine actions for intervention. However, determining the graphical structure of a BN remains a major challenge, especially when modelling a problem under causal assumptions. Solutions to this problem include the automated discovery of BN graphs from data, constructing them based on expert knowledge, or a combination of the two. This paper provides a comprehensive review of combinatoric algorithms proposed for learning BN structure from data, describing 74 algorithms including prototypical, well-established and state-of-the-art approaches. The basic approach of each algorithm is described in consistent terms, and the similarities and differences between them highlighted. Methods of evaluating algorithms and their comparative performance are discussed including the consistency of claims made in the literature. Approaches for dealing with data noise in real-world datasets and incorporating expert knowledge into the learning process are also covered.},
	author = {Neville Kenneth Kitson and Anthony C. Constantinou and Zhigao Guo and Yang Liu and Kiattikun Chobtham},
	doi = {10.1007/s10462-022-10351-w},
	issn = {15737462},
	issue = {8},
	journal = {Artificial Intelligence Review},
	keywords = {Causal discovery,Graphical models,Knowledge-based constraints,Structure learning evaluation,Structure learning review},
	month = {8},
	pages = {8721-8814},
	publisher = {Springer Nature},
	title = {A survey of Bayesian Network structure learning},
	volume = {56},
	year = {2023},
}
@misc{Marcot2019,
	abstract = {Bayesian network (BN) modeling is a rapidly advancing field. Here we explore new methods by which BN model development and application are being joined with other tools and model frameworks. Advances include improving areas of Bayesian classifiers and machine-learning algorithms for model structuring and parameterization, and development of time-dynamic models. Increasingly, BN models are being integrated with: management decision networks; structural equation modeling of causal networks; Bayesian neural networks; combined discrete and continuous variables; object-oriented and agent-based models; state-and-transition models; geographic information systems; quantum probability; and other fields. Integrated BNs (IBNs) are becoming useful tools in risk analysis, risk management, and decision science for resource planning and environmental management. In the near future, IBNs may become self-structuring, self-learning systems fed by real-time monitoring data. Such advances may make model validation difficult, and may question model credibility, particularly if based on uncertain sources of knowledge systems and big data.},
	author = {Bruce G. Marcot and Trent D. Penman},
	doi = {10.1016/j.envsoft.2018.09.016},
	issn = {13648152},
	journal = {Environmental Modelling and Software},
	keywords = {Bayesian networks,Decision models,Machine learning,Model integration,Model validation},
	month = {1},
	pages = {386-393},
	publisher = {Elsevier Ltd},
	title = {Advances in Bayesian network modelling: Integration of modelling technologies},
	volume = {111},
	year = {2019},
}
@misc{,
	abstract = {We present a method for learning Bayesian networks from data sets containing thousands of variables without the need for structure constraints. Our approach is made of two parts. The first is a novel algorithm that effectively explores the space of possible parent sets of a node. It guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time. The second part is an improvement of an existing ordering-based algorithm for structure optimization. The new algorithm provably achieves a higher score compared to its original formulation. Our novel approach consistently outperforms the state of the art on very large data sets.},
	author = {Mauro Scanagatta and Giorgio Corani and Marco Zaffalon},
	title = {Learning Bayesian Networks with Thousands of Variables},
	url = {http://www.cs.york.ac.uk/aig/sw/gobnilp/},
}
@misc{Cooper1992,
	abstract = {This paper presents a Bayesian method for constructing probabilistic networks from databases. In particular , we focus on constructing Bayesian belief networks. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabifistic expert systems. We extend the basic method to handle missing data and hidden (latent) variables. We show how to perform probahilistic inference by averaging over the inferences of multiple belief networks. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. Finally, we relate the methods in this paper to previous work, and we discuss open problems. Keywords. probabilistic networks, Bayesian belief networks, machine learning, induction 1. I n t r o d u c t i o n In this paper, we present a Bayesian method for constructing a probabilistic network from a database of records, which we call cases. Once constructed, such a network can provide insight into probabilistic dependencies that exist among the variables in the database. One application is the automated discovery of dependency relationships. The computer program searches for a probabilistic-network structure that has a high posterior probability given the database, and outputs the structure and its probability. A related task is computer-assisted hypothesis testing: The user enters a hypothetical structure of the dependency relationships among a set of variables, and the program calculates the probability of the structure given a database of cases on the variables. We can also construct a network and use it for computer-based diagnosis. For example, suppose we have a database in which a case contains data about the behavior of some system (i.e., findings). Suppose further that a case contains data about whether this particular behavior follows from proper system operation, or alternatively, is caused by one of several possible faults. Assume that the database contains many such cases from previous episodes of proper and faulty behavior. The method that we present in this paper can be used to construct from the database a probabilistic network that captures the probabilistic dependencies among findings and faults. Such a network then can be applied to classify future cases of system behavior by assigning a posterior probability to each of the possible faults and to the event "proper system operation." In this paper, we also shall discuss diagnostic inference that is based on combining the inferences of multiple alternative networks.},
	author = {Gregory E Cooper},
	pages = {309-347},
	title = {A Bayesian Method for the Induction of Probabilistic Networks from Data},
	volume = {9},
	year = {1992},
}
@inproceedings{Sallard2023,
	abstract = {Thanks to their ability to simulate the travel behavior at the individual scale, agent-based models have gained popularity over the last years. These models are data-intensive, with regards to transport supply and demand. In particular, a detailed description of the population and its travel behavior is required. Bayesian Networks (BNs) are directed acyclic graphs representing joint probability distributions. They have recently been employed for population synthesis and daily activity patterns generation in studies showing that BNs effectively capture the causality links existing between variables and are easily interpretable. Moreover, given their flexible structure, BNs can be adapted for situations in which data from various sources is combined. In this paper, our goal is to estimate a BN for both population and activity pattern synthesis in Switzerland. We evaluate the performance of this approach compared to the statistical matching algorithm using aggregated and disaggregated metrics. In particular, we show that understanding the dependency structure linking the population characteristics and its mobility behavior is key to generate representative synthetic agents and daily activity patterns. This study is a contribution towards the development of interpretable, flexible and behaviorally rich travel demand generation models.},
	author = {Aurore Sallard and Miloš Balac},
	doi = {10.1016/j.procs.2023.03.035},
	issn = {18770509},
	journal = {Procedia Computer Science},
	keywords = {Activity-based models,Bayesian networks,Synthetic Population,Travel demand generation},
	pages = {267-274},
	publisher = {Elsevier B.V.},
	title = {Travel demand generation using Bayesian Networks: An application to Switzerland},
	volume = {220},
	year = {2023},
}
@article{,
	abstract = {To deal with increasing amounts of data, decision and policymakers frequently turn to advances in machine learning and artificial intelligence to capitalise on the potential reward. But there is also a reluctance to trust black-box models, especially when such models are used to support decisions and policies that affect people directly, like those associated with transport and people's mobility. Recent developments focus on explainable artificial intelligence to bolster models’ trustworthiness. In this paper, we demonstrate the use of an explainable-by-design model, Bayesian Networks, on travel behaviour. The model incorporates various demographic and socioeconomic variables to describe full day activity chains: activity and mode choice, as well as the activity and trip durations. More importantly, this paper shows how the model can be used to provide the most relevant explanation for people's observed travel behaviour. The overall goal is to show that model explanations can be quantified and, therefore, assist policymakers to truly make evidence-based decisions. This goal is achieved through two case studies to explain people's vulnerability as it pertains to their total trip duration.},
	author = {Alta de Waal and Johan W. Joubert},
	doi = {10.1016/j.eswa.2022.118348},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	keywords = {Activity chain,Bayesian networks,Explainable artificial intelligence,Most relevant explanation},
	month = {12},
	publisher = {Elsevier Ltd},
	title = {Explainable Bayesian networks applied to transport vulnerability},
	volume = {209},
	year = {2022},
}
@article{Joubert2020,
	abstract = {While activity-based travel demand generation has improved over the last few decades, the behavioural richness and intuitive interpretation remain challenging. This paper argues that it is essential to understand why people travel the way they do and not only be able to predict the overall activity patterns accurately. If one cannot understand the “why?” then a model's ability to evaluate the impact of future interventions is severely diminished. Bayesian networks (BNs) provide the ability to investigate causality and is showing value in recent literature to generate synthetic populations. This paper is novel in extending the application of BNs to daily activity tours. Results show that BNs can synthesise both activity and trip chain structures accurately. It outperforms a frequentist approach and can cater for infrequently observed activity patterns, and patterns unobserved in small sample data. It can also account for temporal variables like activity duration.},
	author = {Johan W. Joubert and Alta de Waal},
	doi = {10.1016/j.trc.2020.102804},
	issn = {0968090X},
	journal = {Transportation Research Part C: Emerging Technologies},
	keywords = {Activity choice,Activity-based,Tour generation,Travel demand},
	month = {11},
	publisher = {Elsevier Ltd},
	title = {Activity-based travel demand generation using Bayesian networks},
	volume = {120},
	year = {2020},
}
@article{Ma2018,
	abstract = {In this work, we propose a Bayesian network approach by using structural restrictions and a model averaging algorithm for modeling the location choice of discretionary activities. In a first stage, we delimit individuals' location choice which is set by generating an ellipse that uses empirical detour factors and a homework axis. The choice set is further refined by an individual's space-time constraints in order to identify the constrained destination choice set. We use structural restrictions and a model averaging method to learn the network structure of the Bayesian network in order to predict the heuristics of individuals' location selection. The empirical study shows the proposed method can effectively obtain Bayesian networks with a consistent dependency structure. The empirical study suggests activity schedule factors significantly influence location choice decisions.},
	author = {Tai-Yu Ma and Sylvain Klein},
	issn = {1567-7141},
	issue = {1},
	journal = {EJTIR Issue},
	keywords = {Bayesian networks,Choice set generation,Location choice,Space-time behaviour,Structure learning},
	pages = {91-111},
	title = {Bayesian networks for constrained location choice modeling using structural restrictions and model averaging},
	volume = {18},
	url = {http://tlo.tbm.tudelft.nl/ejtir},
	year = {2018},
}
@article{Ma2017,
	abstract = {This work contributes to develop a new methodology to identify empirical-data-driven causal structure of a domain knowledge. We propose an algorithm as a two-stage procedure by first drawing relevant prior partial relationships between variables and using them as structure constraints in a structure learning task of Bayesian networks (BNs). The latter is then based on a model averaging approach to obtain a statistically sound BN. The empirical study focuses on modeling commuters' travel mode choice. We present experimental results on testing the design of prior restrictions, the effect of resampling size and learning algorithms, and the effect of random draw on fitted BN structure. The results show that the proposed method can capture more sophisticated relationships between the variables that are missing in both decision tree models and random utility models.},
	author = {Tai Yu Ma and Joseph Y.J. Chow and Jia Xu},
	doi = {10.1080/23249935.2016.1265019},
	issn = {23249943},
	issue = {4},
	journal = {Transportmetrica A: Transport Science},
	keywords = {Bayesian networks,causal structure,structure learning algorithm,travel mode choice},
	month = {4},
	pages = {299-325},
	publisher = {Taylor and Francis Ltd.},
	title = {Causal structure learning for travel mode choice using structural restrictions and model averaging algorithm},
	volume = {13},
	year = {2017},
}
@inproceedings{Ma2015,
	abstract = {Reducing car use and promoting public transport in the cross border area of Luxembourg has become a priority for sustainable development of the Greater region. In this study, we analyze daily mobility mode choice behavior of these cross border workers, in particular, focusing on their multimodal mode choices (e.g. park and ride mode choice) and on their trip chaining behavior. A rule-based approach based on Bayesian networks is proposed to capture the non-linear effects of related determinants/constraints on individuals' mode choice behavior. The result shows the propose Bayesian network has a competitive performance compared with classical discrete choice models with reasonable good corrected prediction rates.},
	author = {Tai Yu Ma},
	doi = {10.1016/j.trpro.2015.09.040},
	issn = {23521465},
	journal = {Transportation Research Procedia},
	keywords = {Luxembourg,bayesian network,mode choice,multimodal,uncertainty},
	pages = {870-880},
	publisher = {Elsevier B.V.},
	title = {Bayesian networks for multimodal mode choice behavior modelling: A case study for the cross border workers of Luxembourg},
	volume = {10},
	year = {2015},
}
@article{Janssens2006,
	abstract = {Several activity-based transportation models are now becoming operational and are entering the stage of application for the modelling of travel demand. Some of these models use decision rules to support its decision-making instead of principles of utility maximization. Decision rules can be derived from different modelling approaches. In a previous study, it was shown that Bayesian networks outperform decision trees and that they are better suited to capture the complexity of the underlying decision-making. However, one of the disadvantages is that Bayesian networks are somewhat limited in terms of interpretation and efficiency when rules are derived from the network, while rules derived from decision trees in general have a simple and direct interpretation. Therefore, in this study, the idea of combining decision trees and Bayesian networks was explored in order to maintain the potential advantages of both techniques. The paper reports the findings of a methodological study that was conducted in the context of Albatross, which is a sequential rule based model of activity scheduling behaviour. To this end, the paper can be situated within the context of a series of previous publications by the authors to improve decision-making in Albatross. The results of this study suggest that integrated Bayesian networks and decision trees can be used for modelling the different choice facets of Albatross with better predictive power than CHAID decision trees. Another conclusion is that there are initial indications that the new way of integrating decision trees and Bayesian networks has produced a decision tree that is structurally more stable. © 2005 Elsevier B.V. All rights reserved.},
	author = {Davy Janssens and Geert Wets and Tom Brijs and Koen Vanhoof and Theo Arentze and Harry Timmermans},
	doi = {10.1016/j.ejor.2005.03.022},
	issn = {03772217},
	issue = {1},
	journal = {European Journal of Operational Research},
	keywords = {Activity-based transportation modelling,BNT classifier,Bayesian networks,Decision trees,Transportation},
	month = {11},
	pages = {16-34},
	title = {Integrating Bayesian networks and decision trees in a sequential rule-based transportation model},
	volume = {175},
	year = {2006},
}