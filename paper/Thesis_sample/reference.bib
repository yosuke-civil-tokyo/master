@book{Nocedal&Wright,
	author = {Jorge Nocedal and Strephan J. Wright},
	doi = {10.1007/978-0-387-40065-5},
	isbn = {978-0-387-30303-1},
	publisher = {Springer New York},
	title = {Numerical Optimization},
	url = {http://link.springer.com/10.1007/978-0-387-40065-5},
	year = {2006}
}
@article{Finn2017,
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression , and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	author = {Chelsea Finn and Pieter Abbeel and Sergey Levine},
	issn = {2640-3498},
	month = {7},
	pages = {1126-1135},
	publisher = {PMLR},
	title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
	url = {https://proceedings.mlr.press/v70/finn17a.html},
	year = {2017}
}
@article{Forrester2008_Explore,
	author = {Alexander I. J. Forrester and Andrs Sbester and Andy J. Keane},
	doi = {10.1002/9780470770801.CH3},
	journal = {Engineering Design via Surrogate Modelling},
	month = {7},
	pages = {77-107},
	publisher = {John Wiley & Sons, Ltd},
	title = {Exploring and Exploiting a Surrogate},
	year = {2008}
}
@article{Forrester2008_Surrogate,
	author = {Alexander I. J. Forrester and Andrs Sbester and Andy J. Keane},
	doi = {10.1002/9780470770801.CH2},
	journal = {Engineering Design via Surrogate Modelling},
	keywords = {engineering design via surrogate modelling,parameter estimation problem,radial basis function models,supervised or instance based learning process,support vector regression (SVR),surrogate based optimization kriging,surrogate construction,surrogate construction process and complexity control problem},
	month = {7},
	pages = {33-76},
	publisher = {John Wiley & Sons, Ltd},
	title = {Constructing a Surrogate},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1002/9780470770801.ch2 https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470770801.ch2 https://onlinelibrary.wiley.com/doi/10.1002/9780470770801.ch2},
	year = {2008}
}
@article{Forrester2008,
	abstract = {Surrogate models expedite the search for promising designs by standing in for expensive design evaluations or simulations. They provide a global model of some metric of a design (such as weight, aerodynamic drag, cost, etc.), which can then be optimized efficiently. Engineering Design via Surrogate Modelling is a self-contained guide to surrogate models and their use in engineering design. The fundamentals of building, selecting, validating, searching and refining a surrogate are presented in a manner accessible to novices in the field. Figures are used liberally to explain the key concepts and clearly show the differences between the various techniques, as well as to emphasize the intuitive nature of the conceptual and mathematical reasoning behind them. More advanced and recent concepts are each presented in stand-alone chapters, allowing the reader to concentrate on material pertinent to their current design problem, and concepts are clearly demonstrated using simple design problems. This collection of advanced concepts (visualization, constraint handling, coping with noisy data, gradient-enhanced modelling, multi-fidelity analysis and multiple objectives) represents an invaluable reference manual for engineers and researchers active in the area. Engineering Design via Surrogate Modelling is complemented by a suite of Matlab codes, allowing the reader to apply all the techniques presented to their own design problems. By applying statistical modelling to engineering design, this book bridges the wide gap between the engineering and statistics communities. It will appeal to postgraduates and researchers across the academic engineering design community as well as practising design engineers.},
	author = {Alexander I. J. Forrester and András Sóbester and Andy J. Keane},
	doi = {10.1002/9780470770801},
	journal = {Engineering Design via Surrogate Modelling},
	month = {7},
	publisher = {Wiley},
	title = {Engineering Design via Surrogate Modelling},
	year = {2008}
}
@article{Forrester2008_Gradient,
	author = {Alexander I. J. Forrester and Andrs Sbester and Andy J. Keane},
	doi = {10.1002/9780470770801.CH7},
	journal = {Engineering Design via Surrogate Modelling},
	keywords = {Algorithmic Differentiation (AD),Branin function with gradient,Hessian enhanced Kriging model,derivative information and lengthier parameter estimation,enhanced Kriging,enhanced Kriging prediction,enhanced modelling,gradient,gradient enhanced Kriging and Hessian,gradient information exploitation,obtaining gradient information},
	month = {7},
	pages = {155-165},
	publisher = {John Wiley & Sons, Ltd},
	title = {Exploiting Gradient Information},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1002/9780470770801.ch7 https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470770801.ch7 https://onlinelibrary.wiley.com/doi/10.1002/9780470770801.ch7},
	year = {2008}
}
@article{Kuwahara2016,
	author = {桑原雅夫・原祐輔・三谷卓摩・川崎洋輔・竹之内篤・井料隆雅，浦田淳司},
	journal = {土木計画学研究・講演集},
	month = {11},
	pages = {54-58},
	publisher = {土木学会},
	url = {https://jglobal.jst.go.jp/detail?JGLOBAL_ID=201702226225812750},
	year = {2016},
	title = {熊本地震における都市内交通，避難の実態と課題}
}
@article{urata2020,
	author = {浦田 淳司・佐々木 泰, 井料隆雅},
	journal = {第61回土木計画学研究発表会},
	publisher = {土木学会},
	year = {2020},
	title = {災害復旧期におけるアクティビティシミュレータの開発と適用}
}

@article{sasaki2019,
	author = {佐々木 泰・浦田 淳司, 井料隆雅},
	journal = {第60回土木計画学研究発表会},
	publisher = {土木学会},
	year = {2019},
	title = {災害復旧期における日常活動需要計算のためのアクティビティシミュレータの構築}
}

@article{Rasouli2014,
	abstract = {Because two decades have almost passed since the introduction of activity-based models of travel demand, this seems the right time to evaluate progress made in the development and application of these models. This invited paper seeks to discuss the initial promises of activity-based models as an alternative to four-step and tour-based models, summarize progress made and identify still unsolved issues that require further research. © 2013 © 2013 The Institute of Urban Sciences.},
	author = {Soora Rasouli and Harry Timmermans},
	doi = {10.1080/12265934.2013.835118},
	issn = {12265934},
	issue = {1},
	journal = {International Journal of Urban Sciences},
	keywords = {activity-based models,transport,travel demand forecasting},
	month = {1},
	pages = {31-60},
	title = {Activity-based models of travel demand: Promises, progress and prospects},
	volume = {18},
	year = {2014}
}
@article{Roorda2008,
	abstract = {The objective of this paper is to verify/validate the results of an application of the Travel Activity Scheduler for Household Agents (TASHA) in the Greater Toronto Area (GTA), Canada. Activity generation and scheduling components of TASHA are validated using 1996 and 2001 travel survey data for the GTA. This validation proceeds in two parts: (a) verification that TASHA replicates the 1996 base case upon which the model was originally built; and (b) comparison of TASHA's forecasts of 2001 daily travel behaviour with observed travel survey data for 2001. TASHA activity generation and scheduling model components replicate observed activities with good accuracy and precision for the base year. Although TASHA is not able to predict an observed increase in activity participation rate in a five year forecast, the distribution of activities in the day is forecast with greater success. Predictions of average travel distance are oversimulated in the base year by 0.9%. Increased average distances are underpredicted for school, shopping, and other activities, and are overpredicted for work and return home activities. These validation results are promising, although there exist opportunities to improve model performance, and to further validate other elements of the TASHA model. © 2007 Elsevier Ltd. All rights reserved.},
	author = {Matthew J. Roorda and Eric J. Miller and Khandker M. Nurul Habib},
	doi = {10.1016/j.tra.2007.10.004},
	issn = {09658564},
	issue = {2},
	journal = {Transportation Research Part A: Policy and Practice},
	keywords = {Activity based modeling,Activity scheduling,Microsimulation,TASHA,Travel demand forecasting,Validation},
	pages = {360-375},
	publisher = {Elsevier Ltd},
	title = {Validation of TASHA: A 24-h activity scheduling microsimulation model},
	volume = {42},
	year = {2008}
}
@article{Kleijnen2005,
	abstract = {Sensitivity analysis may serve validation, optimization, and risk analysis of simulation models. This review surveys 'classic' and 'modern' designs for experiments with simulation models. Classic designs were developed for real, non-simulated systems in agriculture, engineering, etc. These designs assume 'a few' factors (no more than 10 factors) with only 'a few' values per factor (no more than five values). These designs are mostly incomplete factorials (e.g., fractionals). The resulting input/output (I/O) data are analyzed through polynomial metamodels, which are a type of linear regression models. Modern designs were developed for simulated systems in engineering, management science, etc. These designs allow 'many factors (more than 100), each with either a few or 'many' (more than 100) values. These designs include group screening, Latin hypercube sampling (LHS), and other 'space filling' designs. Their I/O data are analyzed through second-order polynomials for group screening, and through Kriging models for LHS. © 2004 Elsevier B.V. All rights reserved.},
	author = {Jack P.C. Kleijnen},
	doi = {10.1016/j.ejor.2004.02.005},
	issn = {03772217},
	issue = {2},
	journal = {European Journal of Operational Research},
	keywords = {Regression,Risk analysis,Scenarios,Simulation,Uncertainty modelling},
	month = {7},
	pages = {287-300},
	publisher = {Elsevier},
	title = {An overview of the design and analysis of simulation experiments for sensitivity analysis},
	volume = {164},
	year = {2005}
}
@article{Wim2004,
	abstract = {Many simulation experiments require much computer time, so they necessitate interpolation for sensitivity analysis and optimization. The interpolating functions are 'metamodels' (or 'response surfaces') of the underlying simulation models. Classic methods combine low-order polynomial regression analysis with fractional factorial designs. Modern Kriging provides 'exact' interpolation, i.e., predicted output values at inputs already observed equal the simulated output values. Such interpolation is attractive in deterministic simulation, and is often applied in Computer Aided Engineering. In discrete-event simulation, however, Kriging has just started. Methodologically, a Kriging metamodel covers the whole experimental area; i.e., it is global (not local). Kriging often gives better global predictions than regression analysis. Technically, Kriging gives more weight to 'neighboring' observations. To estimate the Kriging metamodel, space filling designs are used; for example, Latin Hypercube Sampling (LHS). This paper also presents novel, customized (application driven) sequential designs based on cross-validation and bootstrapping.},
	author = {Wim C.M. Van Beers and Jack P.C. Kleijnen},
	doi = {10.1109/WSC.2004.1371308},
	issn = {08917736},
	journal = {Proceedings - Winter Simulation Conference},
	pages = {113-120},
	title = {Kriging interpolation in simulation: A survey},
	volume = {1},
	year = {2004}
}
@report{Tresidder2012,
	abstract = {This paper describes an experiment to test the performance of Kriging surrogate modelling optimisation techniques on a building design problem with discrete design choices. Surrogate modelling optimisation offers advantages over traditional optimisation techniques on design problems with expensive (time consuming) performance evaluation models. The techniques are tested for both single and multi-objective optimisation problems with the objective of minimising both annual CO 2 emissions predicted by a dynamic simulation and construction cost. The estimated CO 2 emissions and costs of all possible designs were first established through comprehensive analysis using a multi-processor computer, enabling the performance of the optimisation to be assessed precisely against a known single optimum or Pareto front. The performance is compared against an evolutionary algorithm (EA) searching the dynamic simulation model on the same design problem. The results show that for this design problem, Kriging surrogate modelling optimisation is effective at finding estimates of optimum designs. In the case of the single-objective optimisation it is able to find the optimum in fewer simulation calls than the stand-alone EA. In the case of the multi-objective optimisation it is capable of finding a better Pareto front if the total number of simulations is restricted, although the time cost associated with Kriging does not always mean it is worth using.},
	author = {Es Tresidder and Yi Zhang and Alexander I J Forrester},
	title = {ACCELERATION OF BUILDING DESIGN OPTIMISATION THROUGH THE USE OF KRIGING SURROGATE MODELS},
	url = {http://apps1.eere.energy.gov/buildings/energyplus/},
	year = {2012}
}
@article{Bhosekar2018,
	abstract = {The idea of using a simpler surrogate to represent a complex phenomenon has gained increasing popularity over past three decades. Due to their ability to exploit the black-box nature of the problem and the attractive computational simplicity, surrogates have been studied by researchers in multiple scientific and engineering disciplines. Successful use of surrogates shall result in significant savings in terms of computational time and resources. However, with a wide variety of approaches available in the literature, the correct choice of surrogate is a difficult task. An important aspect of this choice is based on the type of problem at hand. This paper reviews recent advances in the area of surrogate models for problems in modeling, feasibility analysis, and optimization. Two of the frequently used surrogates, radial basis functions, and Kriging are tested on a variety of test problems. Finally, guidelines for the choice of appropriate surrogate model are discussed.},
	author = {Atharv Bhosekar and Marianthi Ierapetritou},
	doi = {10.1016/j.compchemeng.2017.09.017},
	issn = {00981354},
	journal = {Computers and Chemical Engineering},
	keywords = {Derivative-free optimization,Feasibility analysis,Model selection,Sampling,Surrogate models},
	month = {1},
	pages = {250-267},
	publisher = {Elsevier Ltd},
	title = {Advances in surrogate based modeling, feasibility analysis, and optimization: A review},
	volume = {108},
	year = {2018}
}
@article{Han2012,
	abstract = {Surrogate-based optimization (Queipo et al. 2005, Simpson et al. 2008) represents a class of optimization methodologies that make use of surrogate modeling techniques to quickly find the local or global optima. It provides us a novel optimization framework in which the conventional optimization algorithms, e.g. gradient-based or evolutionary algorithms are used for sub-optimization(s). Surrogate modeling techniques are of particular interest for engineering design when high-fidelity, thus expensive analysis codes (e.g. Computation Fluid Dynamics (CFD) or Computational Structural Dynamics (CSD)) are used. They can be used to greatly improve the design efficiency and be very helpful in finding global optima, filtering numerical noise, realizing parallel design optimization and integrating simulation codes of different disciplines into a process chain. Here the term “surrogate model” has the same meaning as “response surface model”, “metamodel”, “approximation model”, “emulator” etc. This chapter aims to give an overview of existing surrogate modeling techniques and issues about how to use them for optimization.},
	author = {Zhong-Hua Han and Ke-Shi Zhang},
	doi = {10.5772/36125},
	journal = {Real-World Applications of Genetic Algorithms},
	month = {3},
	publisher = {InTech},
	title = {Surrogate-Based Optimization},
	year = {2012}
}
@article{Kim2020,
	abstract = {Simulation-based optimization using surrogate models enables decision-making through the exchange of data from high-fidelity models and development of approximations. Many chemical engineering optimization problems, such as process design and synthesis, rely on simulations and contain both discrete and continuous decision variables. Surrogate-based optimization with continuous variables has been studied extensively; however, there are many open challenges for the case of mixed-variable inputs. In this work, we propose an algorithm for mixed-integer nonlinear simulation-based problems that uses adaptive sampling and surrogate modeling with one-hot encoding. We propose techniques for the design of experiments for mixed-variable problems, surrogate modeling for mixed-variable response surfaces, and iterative approximation-optimization procedure that leads to optimal solutions. Results show that one-hot encoding leads to accurate and robust mixed-variable Gaussian Process and Neural Network models that are effective surrogates for optimization. The proposed algorithm is tested on mixed-integer nonlinear benchmark problems and a chemical process synthesis case study.},
	author = {Sun Hye Kim and Fani Boukouvala},
	doi = {10.1016/j.compchemeng.2020.106847},
	issn = {00981354},
	journal = {Computers and Chemical Engineering},
	month = {9},
	publisher = {Elsevier Ltd},
	title = {Surrogate-based optimization for mixed-integer nonlinear problems},
	volume = {140},
	year = {2020}
}
@article{Baydin2018,
	abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
	author = {Atılım Güne¸ and Güne¸s Baydin and Barak A Pearlmutter and Jeffrey Mark Siskind},
	journal = {Journal of Machine Learning Research},
	keywords = {Backpropagation,Differentiable Programming Baydin,Pearlmutter,Radul,and Siskind},
	pages = {1-43},
	title = {Automatic Differentiation in Machine Learning: a Survey},
	volume = {18},
	year = {2018}
}
@article{Rios2013,
	abstract = {This paper addresses the solution of bound-constrained optimization problems using algorithms that require only the availability of objective function values but no derivative information. We refer to these algorithms as derivative-free algorithms. Fueled by a growing number of applications in science and engineering, the development of derivative-free optimization algorithms has long been studied, and it has found renewed interest in recent time. Along with many derivative-free algorithms, many software implementations have also appeared. The paper presents a review of derivative-free algorithms, followed by a systematic comparison of 22 related implementations using a test set of 502 problems. The test bed includes convex and nonconvex problems, smooth as well as nonsmooth problems. The algorithms were tested under the same conditions and ranked under several criteria, including their ability to find near-global solutions for nonconvex problems, improve a given starting point, and refine a near-optimal solution. A total of 112,448 problem instances were solved. We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size. For the problems used in this study, TOMLAB/MULTIMIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO are better, on average, than other derivative-free solvers in terms of solution quality within 2,500 function evaluations. These global solvers outperform local solvers even for convex problems. Finally, TOMLAB/OQNLP, NEWUOA, and TOMLAB/MULTIMIN show superior performance in terms of refining a near-optimal solution. © 2012 Springer Science+Business Media, LLC.},
	author = {Luis Miguel Rios and Nikolaos V. Sahinidis},
	doi = {10.1007/s10898-012-9951-y},
	issn = {09255001},
	issue = {3},
	journal = {Journal of Global Optimization},
	keywords = {Derivative-free algorithms,Direct search methods,Surrogate models},
	month = {7},
	pages = {1247-1293},
	title = {Derivative-free optimization: A review of algorithms and comparison of software implementations},
	volume = {56},
	year = {2013}
}
@article{Quentin2021,
	abstract = {Simulating frictional contacts remains a challenging research topic in robotics. Recently, differentiable physics emerged and has proven to be a key element in model-based Reinforcement Learning (RL) and optimal control fields. However, most of the current formulations deploy coarse approximations of the underlying physical principles. Indeed, the classic simulators loose precision by casting the Nonlinear Complementarity Problem (NCP) of frictional contact into a Linear Complementarity Problem (LCP) to simplify computations. Moreover, such methods deploy non-smooth operations and cannot be automatically differentiated. In this letter, we propose (i) an extension of the staggered projections algorithm for more accurate solutions of the problem of contacts with friction. Based on this formulation, we introduce (ii) a differentiable simulator and an efficient way to compute the analytical derivatives of the involved optimization problems. Finally, (iii) we validate the proposed framework with a set of experiments to present a possible application of our differentiable simulator. In particular, using our approach we demonstrate accurate estimation of friction coefficients and object masses both in synthetic and real experiments.},
	author = {Quentin Le Lidec and Igor Kalevatykh and Ivan Laptev and Cordelia Schmid and Justin Carpentier},
	doi = {10.1109/LRA.2021.3062323},
	issn = {23773766},
	issue = {2},
	journal = {IEEE Robotics and Automation Letters},
	keywords = {Calibration and identification,contact modeling,optimization and optimal control,simulation and animation},
	month = {4},
	pages = {3413-3420},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {Differentiable Simulation for Physical System Identification},
	volume = {6},
	year = {2021}
}

@article{Katoch2021,
	abstract = {In this paper, the analysis of recent advances in genetic algorithms is discussed. The genetic algorithms of great interest in research community are selected for analysis. This review will help the new and demanding researchers to provide the wider vision of genetic algorithms. The well-known algorithms and their implementation are presented with their pros and cons. The genetic operators and their usages are discussed with the aim of facilitating new researchers. The different research domains involved in genetic algorithms are covered. The future research directions in the area of genetic operators, fitness function and hybrid algorithms are discussed. This structured review will be helpful for research and graduate teaching.},
	author = {Sourabh Katoch and Sumit Singh Chauhan and Vijay Kumar},
	doi = {10.1007/s11042-020-10139-6},
	issn = {15737721},
	issue = {5},
	journal = {Multimedia Tools and Applications},
	keywords = {Crossover,Evolution,Genetic algorithm,Metaheuristic,Mutation,Optimization,Selection},
	month = {2},
	pages = {8091-8126},
	publisher = {Springer},
	title = {A review on genetic algorithm: past, present, and future},
	volume = {80},
	year = {2021}
}
@article{Mustapha2020,
	abstract = {Maximizing or minimizing a function is a problem in several areas. In computer science and for systems based on Machine Learning (ML), a panoply of optimization algorithms makes it possible to grasp the main learning bases, particularly in terms of features number, and this by reducing the volume of data to be kept in memory while producing satisfactory results. Among these algorithms, the different variants of the gradient descent algorithm which is widely used in ML. This paper presents a comparative study of batch, stochastic and mini-batch gradient descent algorithms as well as the normal equation algorithm of optimization, this study will facilitate the choice of the appropriate optimization algorithm to adopt when building a system based on ML. The case study implemented in this work is based on the keratoconus dataset of Harvard Dataverse. The obtained results show that stochastic and mini-batch gradient descent algorithms represent the best performances than the other algorithms, particularly for systems based on ML involving a high number of variables.},
	author = {Aatila Mustapha and Lachgar Mohamed and Kartit Ali},
	doi = {10.1007/978-3-030-45183-7_27},
	isbn = {9783030451820},
	issn = {18650937},
	journal = {Communications in Computer and Information Science},
	keywords = {Batch gradient descent,Mini-batch gradient descent,Normal equation,Optimization,Stochastic gradient descent},
	pages = {349-359},
	publisher = {Springer},
	title = {An Overview of Gradient Descent Algorithm Optimization in Machine Learning: Application in the Ophthalmology Field},
	volume = {1207 CCIS},
	year = {2020}
}
@inproceedings{Rehbach2020,
	abstract = {Real-world problems such as computational fluid dynamics simulations and finite element analyses are computationally expensive. A standard approach to mitigating the high computational expense is Surrogate-Based Optimization (SBO). Yet, due to the high-dimensionality of many simulation problems, SBO is not directly applicable or not efficient. Reducing the dimensionality of the search space is one method to overcome this limitation. In addition to the applicability of SBO, dimensionality reduction enables easier data handling and improved data and model interpretability. Regularization is considered as one state-of-the-art technique for dimensionality reduction. We propose a hybridization approach called Regularized-Surrogate-Optimization (RSO) aimed at overcoming difficulties related to high-dimensionality. It couples standard Kriging-based SBO with regularization techniques. The employed regularization methods are based on three adaptations of the least absolute shrinkage and selection operator (LASSO). In addition, tree-based methods are analyzed as an alternative variable selection method. An extensive study is performed on a set of artificial test functions and two real-world applications: the electrostatic precipitator problem and a multilayered composite design problem. Experiments reveal that RSO requires significantly less time than standard SBO to obtain comparable results. The pros and cons of the RSO approach are discussed, and recommendations for practitioners are presented.},
	author = {Frederik Rehbach and Lorenzo Gentile and Thomas Bartz-Beielstein},
	doi = {10.1145/3377930.3390195},
	isbn = {9781450371285},
	journal = {GECCO 2020 - Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
	keywords = {Dimensionality reduction,LASSO,Modeling,Real-world,Surrogate-based optimization,Surrogates},
	month = {6},
	pages = {1177-1185},
	publisher = {Association for Computing Machinery},
	title = {Variable reduction for surrogate-based optimization},
	year = {2020}
}
@article{Yang2016,
	abstract = {Surrogate-based Optimization is a useful approach when the objective function is computationally expensive to evaluate, compared to Simulation-based Optimization. In the surrogate-based method, analytically tractable 'surrogate models' (also known as 'Response Surface Models - RSMs' or 'metamodels'), are constructed and validated for each optimization objective and constraint at relatively low computational cost. They are useful for replacing the time-consuming simulations during the optimization; quickly locating the area where the optimum is expected to be for further search; and gaining insight into the global behavior of the system. Nevertheless, there are still concerns about the surrogate model accuracy and the number of simulations necessary to get a reasonably accurate surrogate model. This paper aims to unveil: 1) the possible impacts of problem scale and sampling strategy on the surrogate model accuracy; and 2) the potential of Surrogatebased Optimization in finding high quality solutions for building envelope design optimization problems. For this purpose, a series of multi-objective optimization test cases that mainly consider daylight and energy performance were conducted within the same time frame. Then, the results were compared, in pair, based on which discussions were made. Finally, the corresponding conclusions were obtained after the comparative study.},
	author = {Ding Yang and Yimin Sun and Danilo Di Stefano and Michela Turrin and Sevil Sariyildiz},
	doi = {10.1109/CEC.2016.7744323},
	isbn = {9781509006229},
	journal = {2016 IEEE Congress on Evolutionary Computation, CEC 2016},
	keywords = {Design of experiments,Multi-objective optimization,Problem scale,Response surface model,Sampling strategy,Surrogate-based optimization},
	month = {11},
	pages = {4199-4207},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {Impacts of problem scale and sampling strategy on surrogate model accuracy: An application of surrogate-based optimization in building design},
	year = {2016}
}
@generic{Zhuang2021,
	abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
	author = {Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},
	doi = {10.1109/JPROC.2020.3004555},
	issn = {15582256},
	issue = {1},
	journal = {Proceedings of the IEEE},
	keywords = {Domain adaptation,interpretation,machine learning,transfer learning},
	month = {1},
	pages = {43-76},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {A Comprehensive Survey on Transfer Learning},
	volume = {109},
	year = {2021}
}
@article{Weiss2016,
	abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
	author = {Karl Weiss and Taghi M. Khoshgoftaar and Ding Ding Wang},
	doi = {10.1186/s40537-016-0043-6},
	issn = {21961115},
	issue = {1},
	journal = {Journal of Big Data},
	keywords = {Data mining,Domain adaptation,Machine learning,Survey,Transfer learning},
	month = {12},
	publisher = {SpringerOpen},
	title = {A survey of transfer learning},
	volume = {3},
	year = {2016}
}
@report{Vilalta2002,
	abstract = {Different researchers hold different views of what the term meta-learning exactly means. The first part of this paper provides our own perspective view in which the goal is to build self-adaptive learners (i.e. learning algorithms that improve their bias dynamically through experience by accumulating meta-knowledge). The second part provides a survey of meta-learning as reported by the machine-learning literature. We find that, despite different views and research lines, a question remains constant: how can we exploit knowledge about learning (i.e. meta-knowledge) to improve the performance of learning algorithms? Clearly the answer to this question is key to the advancement of the field and continues being the subject of intensive research.},
	author = {Ricardo Vilalta and Youssef Drissi},
	journal = {Artificial Intelligence Review},
	keywords = {classification,inductive learning,meta-knowledge},
	pages = {77-95},
	title = {A Perspective View and Survey of Meta-Learning},
	volume = {18},
	year = {2002}
}
@article{Zhao2014,
	abstract = {In this paper, we propose a novel machine learning framework called "Online Transfer Learning" (OTL), which aims to attack an online learning task on a target domain by transferring knowledge from some source domain. We do not assume data in the target domain follows the same distribution as that in the source domain, and the motivation of our work is to enhance a supervised online learning task on a target domain by exploiting the existing knowledge that had been learnt from training data in source domains. OTL is in general very challenging since data in both source and target domains not only can be different in their class distributions, but also can be diverse in their feature representations. As a first attempt to this new research problem, we investigate two different settings of OTL: (i) OTL on homogeneous domains of common feature space, and (ii) OTL across heterogeneous domains of different feature spaces. For each setting, we propose effective OTL algorithms to solve online classification tasks, and show some theoretical bounds of the algorithms. In addition, we also apply the OTL technique to attack the challenging online learning tasks with concept-drifting data streams. Finally, we conduct extensive empirical studies on a comprehensive testbed, in which encouraging results validate the efficacy of our techniques. © 2014 Elsevier B.V.},
	author = {Peilin Zhao and Steven C.H. Hoi and Jialei Wang and Bin Li},
	doi = {10.1016/j.artint.2014.06.003},
	issn = {00043702},
	journal = {Artificial Intelligence},
	keywords = {Knowledge transfer,Online learning,Transfer learning},
	pages = {76-102},
	publisher = {Elsevier},
	title = {Online transfer learning},
	volume = {216},
	year = {2014}
}
@article{Hoi2021,
	abstract = {Online learning represents a family of machine learning methods, where a learner attempts to tackle some predictive (or any type of decision-making) task by learning from a sequence of data instances one by one at each time. The goal of online learning is to maximize the accuracy/correctness for the sequence of predictions/decisions made by the online learner given the knowledge of correct answers to previous prediction/learning tasks and possibly additional information. This is in contrast to traditional batch or offline machine learning methods that are often designed to learn a model from the entire training data set at once. Online learning has become a promising technique for learning from continuous streams of data in many real-world applications. This survey aims to provide a comprehensive survey of the online machine learning literature through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the types of learning tasks and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) online supervised learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) online unsupervised learning where no feedback is available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
	author = {Steven C.H. Hoi and Doyen Sahoo and Jing Lu and Peilin Zhao},
	doi = {10.1016/j.neucom.2021.04.112},
	issn = {18728286},
	journal = {Neurocomputing},
	keywords = {Online convex optimization,Online learning,Sequential decision making},
	month = {10},
	pages = {249-289},
	publisher = {Elsevier B.V.},
	title = {Online learning: A comprehensive survey},
	volume = {459},
	year = {2021}
}
@article{Chen2021,
	abstract = {Partial differential equations (PDEs) are ubiquitous in natural science and engineering problems. Traditional discrete methods for solving PDEs are usually time-consuming and labor-intensive due to the need for tedious mesh generation and numerical iterations. Recently, deep neural networks have shown new promise in cost-effective surrogate modeling because of their universal function approximation abilities. In this paper, we borrow the idea from physics-informed neural networks (PINNs) and propose an improved data-free surrogate model, DFS-Net. Specifically, we devise an attention-based neural structure containing a weighting mechanism to alleviate the problem of unstable or inaccurate predictions by PINNs. The proposed DFS-Net takes expanded spatial and temporal coordinates as the input and directly outputs the observables (quantities of interest). It approximates the PDE solution by minimizing the weighted residuals of the governing equations and data-fit terms, where no simulation or measured data are needed. The experimental results demonstrate that DFS-Net offers a good trade-off between accuracy and efficiency. It outperforms the widely used surrogate models in terms of prediction performance on different numerical benchmarks, including the Helmholtz, Klein–Gordon, and Navier–Stokes equations.},
	author = {Xinhai Chen and Rongliang Chen and Qian Wan and Rui Xu and Jie Liu},
	doi = {10.1038/s41598-021-99037-x},
	issn = {20452322},
	issue = {1},
	journal = {Scientific Reports},
	month = {12},
	pmid = {34593943},
	publisher = {Nature Research},
	title = {An improved data-free surrogate model for solving partial differential equations using deep neural networks},
	volume = {11},
	year = {2021}
}
@article{Kitano1992,
	author = {北野 宏明},
	doi = {https://doi.org/10.11517/jjsai.7.1_26},
	issn = {09128085},
	journal = {人工知能学会誌},
	pages = {26-37},
	publisher = {人工知能学会},
	title = {遺伝的アルゴリズム},
	volume = {7},
	year = {1992}
}
@article{Ruder2016,
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	author = {Sebastian Ruder},
	month = {9},
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	year = {2016}
}
@article{Nielsen1989,
	abstract = {The author presents a survey of the basic theory of the backpropagation neural network architecture covering architectural design, performance measurement, function approximation capability, and learning. The survey includes previously known material, as well as some new results, namely, a formulation of the backpropagation neural network architecture to make it a valid neural network (past formulations violated the locality of processing restriction) and a proof that the backpropagation mean-squared-error function exists and is differentiable. Also included is a theorem showing that any L2 function from [0,1]n to Rm can be implemented to any desired degree of accuracy with a three-layer backpropagation neural network. The author presents a speculative neurophysiological model illustrating how the backpropagation neural network architecture might plausibly be implemented in the mammalian brain for corticocortical learning between nearby regions of the cerebral cortex.},
	author = {Robert Hecht-Nielsen},
	doi = {10.1109/IJCNN.1989.118638},
	pages = {593-605},
	publisher = {Publ by IEEE},
	title = {Theory of the backpropagation neural network},
	year = {1989}
}
@article{Caruana1997,
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	author = {Rich Caruana and Lorien Pratt and Sebastian Thrun},
	doi = {10.1023/A:1007379606734},
	issn = {1573-0565},
	issue = {1},
	journal = {Machine Learning 1997 28:1},
	keywords = {Artificial Intelligence,Control,Mechatronics,Natural Language Processing (NLP),Parallel transfer,Robotics,Simulation and Modeling,Supervised learning},
	pages = {41-75},
	publisher = {Springer},
	title = {Multitask Learning},
	volume = {28},
	url = {https://link.springer.com/article/10.1023/A:1007379606734},
	year = {1997}
}
@article{Zinkevich2003,
	author={Martin Zinkevich},
	journal = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
	pages = {928-935},
	publisher = {ICML},
	title = {Online convex programming and generalized infinitesimal gradient ascent},
	url = {https://people.eecs.berkeley.edu/~brecht/cs294docs/week1/03.Zinkevich.pdf},
	year = {2003}
}
@article{Pan2010,
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research. © 2006 IEEE.},
	author = {Sinno Jialin Pan and Qiang Yang},
	doi = {10.1109/TKDE.2009.191},
	issn = {10414347},
	issue = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Transfer learning,data mining.,machine learning,survey},
	pages = {1345-1359},
	title = {A survey on transfer learning},
	volume = {22},
	year = {2010}
}
@article{COELLO2002,
	author = {Carlos A Coello Coello},
	journal = {Comput. Methods Appl. Mech. Engrg.},
	pages = {1245-1287},
	title = {Theoretical and Numerical Constraint-Handling Techniques used with Evolutionary Algorithm : a Survey of the State of the Art},
	volume = {191},
	url = {https://ci.nii.ac.jp/naid/80012794359},
	year = {2002}
}